{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import dgl\n",
    "from model import Model\n",
    "from utils import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import random\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy.io as sio\n",
    "import datetime\n",
    "from ggda import *\n",
    "from typing import List\n",
    "\n",
    "from dgl.nn.pytorch import SGConv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.cuda = True\n",
    "        \n",
    "        self.dataset = \"cora\"\n",
    "        self.device = \"cuda:0\"\n",
    "        self.embedding_dim = 64\n",
    "        \n",
    "        self.n_ggd_epochs = 600\n",
    "        self.patience = 500\n",
    "        self.batch_size = 300\n",
    "        self.eval_freq = 1\n",
    "        \n",
    "        self.n_hidden = 256\n",
    "        self.n_layers = 1\n",
    "        self.dropout = 0\n",
    "        self.proj_layers = 0\n",
    "        self.gnn_encoder = 'gcn'\n",
    "        self.num_hop = 10\n",
    "        self.ggd_lr = 1e-3\n",
    "        self.weight_decay = 0.\n",
    "        \n",
    "        self.subgraph_size = 4\n",
    "        self.auc_test_rounds = 64\n",
    "        \n",
    "        self.neg_batch_size = 1024\n",
    "args =  Args()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def aug_feature_dropout(input_feat, drop_percent=0.2):\n",
    "    # aug_input_feat = copy.deepcopy((input_feat.squeeze(0)))\n",
    "    aug_input_feat = copy.deepcopy(input_feat)\n",
    "    drop_feat_num = int(aug_input_feat.shape[1] * drop_percent)\n",
    "    drop_idx = random.sample([i for i in range(aug_input_feat.shape[1])], drop_feat_num)\n",
    "    aug_input_feat[:, drop_idx] = 0\n",
    "    \n",
    "    return aug_input_feat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Dataset: cora\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "G:\\GNN\\Graph-Group-Discrimination-main\\venv\\lib\\site-packages\\dgl\\base.py:45: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  return warnings.warn(message, category=category, stacklevel=1)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print('Dataset: {}'.format(args.dataset), flush=True)\n",
    "device = torch.device(args.device if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "batch_size = args.batch_size\n",
    "\n",
    "adj, features, labels, idx_train, idx_val, idx_test, ano_label, str_ano_label, attr_ano_label = load_mat(args.dataset)\n",
    "\n",
    "features, _ = preprocess_features(features)\n",
    "src, dst = np.nonzero(adj)\n",
    "g = dgl.graph((src, dst))\n",
    "g.ndata['feat'] = torch.FloatTensor(features)\n",
    "g.ndata['label'] = torch.LongTensor(labels)\n",
    "n_edges = g.number_of_edges()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "nb_nodes = features.shape[0]\n",
    "ft_size = features.shape[1]\n",
    "nb_classes = labels.shape[1]\n",
    "\n",
    "# adj = normalize_adj(adj)\n",
    "# adj = (adj + sp.eye(adj.shape[0])).todense()\n",
    "# adj = torch.FloatTensor(adj[np.newaxis]).to(device)\n",
    "\n",
    "features = torch.FloatTensor(features).to(device)\n",
    "labels = torch.FloatTensor(labels).to(device)\n",
    "idx_train = torch.LongTensor(idx_train).to(device)\n",
    "idx_val = torch.LongTensor(idx_val).to(device)\n",
    "idx_test = torch.LongTensor(idx_test).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create GGD model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Training:  41%|██████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                           | 248/600 [00:08<00:12, 28.50it/s, loss=0.86]\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c34d58a4d825>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[0mcomp_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m                 \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[0mbest_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.bilinear = nn.Bilinear(n_hidden, n_hidden, 1)\n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Bilinear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, features, summary):\n",
    "        s = self.bilinear(features, summary)\n",
    "        return s\n",
    "\n",
    "# class DiscriminatorCos(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(DiscriminatorCos, self).__init__()\n",
    "#         self.cos = torch.nn.CosineSimilarity(dim=1, eps=1e-06)\n",
    "# \n",
    "#     def forward(self, features, summary):\n",
    "#         print(features.shape)\n",
    "#         print(summary.shape)\n",
    "#         s = self.cos(features, summary)\n",
    "#         \n",
    "#         return torch.unsqueeze(s, 0)\n",
    "    \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, g, in_feats, n_hidden, n_layers, activation, dropout, gnn_encoder, k=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.g = g\n",
    "        self.gnn_encoder = gnn_encoder\n",
    "        if gnn_encoder == 'gcn':\n",
    "            self.conv = GCN(g, in_feats, n_hidden, n_hidden, n_layers, activation, dropout)\n",
    "        elif gnn_encoder == 'sgc':\n",
    "            self.conv = SGConv(in_feats, n_hidden, k=10, cached=True)\n",
    "\n",
    "    def forward(self, features):\n",
    "        if self.gnn_encoder == 'gcn':\n",
    "            features = self.conv(features)\n",
    "        elif self.gnn_encoder == 'sgc':\n",
    "            features = self.conv(self.g, features)\n",
    "        return features\n",
    "class GraphLocalGraphPooling(nn.Module):\n",
    "    def __init__(self, g, n_hop):\n",
    "        # TODO: Simulate random walk (randomly drop some subgraph)\n",
    "        super(GraphLocalGraphPooling, self).__init__()\n",
    "        A = g.adjacency_matrix().to_dense() \n",
    "        A = A + torch.eye(A.shape[0])\n",
    "        A_n = A\n",
    "        for i in range(n_hop):\n",
    "            A_n =  torch.matmul(A_n, A)\n",
    "        # TODO: Check matrix situation (sym, factor\n",
    "        A = torch.sign(A_n)\n",
    "        self.A = torch.matmul(torch.diag(1/torch.sum(A, dim=1)), A)\n",
    "        self.A = self.A.cuda()\n",
    "    def forward(self, feature):\n",
    "        # feature: [n_nodes, n_features]\n",
    "        feature = torch.matmul(self.A, feature)\n",
    "        return feature\n",
    "\n",
    "\n",
    "class GGD_Anomaly(nn.Module):\n",
    "    def __init__(self, g, in_feats, n_hidden, n_layers, activation, dropout, proj_layers, gnn_encoder, num_hop, subgraph_size):\n",
    "        super(GGD_Anomaly, self).__init__()\n",
    "        self.g = g\n",
    "        self.encoder = Encoder(g, in_feats, n_hidden, n_layers, activation, dropout, gnn_encoder, num_hop)\n",
    "        self.discriminator = Discriminator(n_hidden)\n",
    "        # self.discriminator = DiscriminatorCos()\n",
    "        self.graph_average_pooling = lambda x: x\n",
    "        if subgraph_size > 0:\n",
    "            self.graph_average_pooling = GraphLocalGraphPooling(g, subgraph_size)\n",
    "        \n",
    "        self.graph_conv_layers = self.encoder.conv.layers\n",
    "        self.mlp = torch.nn.ModuleList()\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        for i in range(proj_layers):\n",
    "            self.mlp.append(nn.Linear(n_hidden, n_hidden))\n",
    "        # GGD\n",
    "        self.lin = nn.Linear(n_hidden, n_hidden)\n",
    "    def forward(self, features):\n",
    "        features = self.dropout(features)\n",
    "        embedding_node = features\n",
    "        for i, graph_conv_layer in enumerate(self.graph_conv_layers):\n",
    "             embedding_node = graph_conv_layer._activation(torch.matmul(embedding_node, graph_conv_layer.weight) + graph_conv_layer.bias)\n",
    "\n",
    "\n",
    "        embedding_graph_pos = self.encoder(features)\n",
    "        # avg pooling \n",
    "        embedding_graph_readout = self.graph_average_pooling(embedding_node)\n",
    "        # Add skip connection\n",
    "        embedding_graph_proj = (embedding_graph_pos + embedding_graph_readout) / 2\n",
    "        # Positive branch of Anomaly\n",
    "        predicted_score_pos = self.discriminator(embedding_node, embedding_graph_proj)\n",
    "        # change shape from [n_nodes, 1] to [1, n_nodes]\n",
    "        predicted_score_pos = torch.swapaxes(predicted_score_pos, 0, 1)\n",
    "        \n",
    "        # Negative branch of Anomaly\n",
    "        perm = torch.randperm(self.g.number_of_nodes())\n",
    "        embedding_node_neg = embedding_node[perm]\n",
    "        predicted_score_neg = self.discriminator(embedding_node_neg, embedding_graph_proj)\n",
    "        predicted_score_neg = torch.swapaxes(predicted_score_neg, 0, 1)\n",
    "        \n",
    "        # ggd \n",
    "        ggd_score_pos = self.lin(embedding_graph_proj).sum(1).unsqueeze(0)\n",
    "        \n",
    "        embedding_graph_neg = self.encoder(features[perm])\n",
    "        ggd_score_neg = self.lin(embedding_graph_neg).sum(1).unsqueeze(0)\n",
    "        return predicted_score_pos, predicted_score_neg, ggd_score_pos, ggd_score_neg\n",
    "g = g.to(device)\n",
    "# Create GGD model\n",
    "\n",
    "gamma = 0.07\n",
    "\n",
    "for i in range(5):\n",
    "    ggd = GGD_Anomaly(\n",
    "        g,\n",
    "        ft_size,\n",
    "        args.n_hidden,\n",
    "        args.n_layers,\n",
    "        nn.PReLU(args.n_hidden),\n",
    "        args.dropout,\n",
    "        args.proj_layers,\n",
    "        args.gnn_encoder,\n",
    "        args.num_hop,\n",
    "        args.subgraph_size\n",
    "    )\n",
    "    if args.cuda:\n",
    "        ggd.cuda()\n",
    "    \n",
    "    #%%\n",
    "    \n",
    "    \n",
    "    ggd_optimizer = torch.optim.Adam(ggd.parameters(),\n",
    "                                     lr=args.ggd_lr,\n",
    "                                     weight_decay=args.weight_decay)\n",
    "    b_xent = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    #%% md\n",
    "    \n",
    "    # train GGD\n",
    "    \n",
    "    #%%\n",
    "    \n",
    "    cnt_wait = 0\n",
    "    best = 1e9\n",
    "    best_t = 0\n",
    "    counts = 0\n",
    "    avg_time = 0\n",
    "    dur = []\n",
    "    loss_list = []\n",
    "    \n",
    "    tag = str(datetime.datetime.now().strftime(\"%m-%d %H%M%S\"))\n",
    "    # print(\"Memory beg:\", torch.cuda.memory_allocated(device) / 1024 / 1024)\n",
    "    \n",
    "    epoch_list = []\n",
    "    auc_score_list = []\n",
    "    auc_pos_list = []\n",
    "    auc_neg_list = []\n",
    "    pos_std = []\n",
    "    neg_std = []\n",
    "    score_std = []\n",
    "    label_positive = torch.zeros(1, g.number_of_nodes()).cuda()\n",
    "    label_negative = torch.ones(1, g.number_of_nodes()).cuda()\n",
    "    with tqdm(total=args.n_ggd_epochs) as pbar:\n",
    "        pbar.set_description('Training')\n",
    "        for epoch in range(args.n_ggd_epochs):\n",
    "            if epoch % args.eval_freq == 0:\n",
    "                ggd.eval()\n",
    "                with torch.no_grad():\n",
    "                    pos_prob_list = []\n",
    "                    neg_prob_list = []\n",
    "                    # for i in range(args.auc_test_rounds):\n",
    "                    #     feature_dropout = aug_feature_dropout(features, 0.2)\n",
    "                    #     pos_prob_list.append(ggd(feature_dropout).detach()[0])\n",
    "                    #     perm = torch.randperm(g.number_of_nodes())\n",
    "                    #     inverse_perm = torch.argsort(perm)\n",
    "                    #     features_perm = feature_dropout[perm]\n",
    "                    #     neg_prob_list.append(ggd(features_perm).detach()[0][inverse_perm])\n",
    "                    pos_prob_list.append(ggd(features)[0].detach()[0])\n",
    "                    # perm = torch.randperm(g.number_of_nodes())\n",
    "                    # inverse_perm = torch.argsort(perm)\n",
    "                    # features_perm = features[perm]\n",
    "                    # neg_prob_list.append(ggd(features_perm).detach()[0])\n",
    "                    # neg_prob_list.append(ggd(features_perm).detach()[0][inverse_perm])\n",
    "                    \n",
    "                    pos_prob = torch.mean(torch.stack(pos_prob_list), axis=0)\n",
    "                    # neg_prob = torch.mean(torch.stack(neg_prob_list), axis=0)\n",
    "                    # ano_score = (neg_prob - pos_prob).cpu().numpy()\n",
    "                    epoch_list.append(epoch)\n",
    "                    # auc_score_list.append(roc_auc_score(ano_label, ano_score))\n",
    "                    auc_pos_list.append(roc_auc_score(ano_label, pos_prob.cpu().numpy()))\n",
    "                    # auc_neg_list.append(roc_auc_score(ano_label, neg_prob.cpu().numpy()))\n",
    "                    pos_std.append(np.std(pos_prob.cpu().numpy()))\n",
    "                    # neg_std.append(np.std(neg_prob.cpu().numpy()))\n",
    "                    # score_std.append(np.std(ano_score))\n",
    "            \n",
    "            t0 = time.time()\n",
    "            ggd.train()\n",
    "            if epoch >= 3:\n",
    "                t0 = time.time()\n",
    "                \n",
    "            ggd_optimizer.zero_grad()\n",
    "            # Positive\n",
    "            # training_features = aug_feature_dropout(features, drop_percent=0.2)\n",
    "            training_features = features\n",
    "            s_positive, s_negative, ggd_score_pos, ggd_score_neg = ggd(training_features)\n",
    "            loss_positive = b_xent(s_positive, label_positive)\n",
    "            loss_negative = b_xent(s_negative, label_negative)\n",
    "            loss_anomaly = loss_positive + loss_negative\n",
    "            loss_ggd = b_xent(ggd_score_pos, label_positive) + b_xent(ggd_score_neg, label_negative)\n",
    "            loss = (1-gamma) * loss_anomaly + gamma * loss_ggd\n",
    "            \n",
    "            loss.backward()\n",
    "            ggd_optimizer.step()\n",
    "        \n",
    "            comp_time = time.time() - t0\n",
    "            if loss < best:\n",
    "                best = loss\n",
    "                best_t = epoch\n",
    "                cnt_wait = 0\n",
    "                # torch.save(ggd.state_dict(), 'checkpoints_ggd/best_ggd' + tag + '.pkl')\n",
    "            else:\n",
    "                cnt_wait += 1\n",
    "        \n",
    "            if cnt_wait == args.patience:\n",
    "                print('Early stopping!')\n",
    "                break\n",
    "        \n",
    "            if epoch >= 3:\n",
    "                dur.append(time.time() - t0)\n",
    "            \n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)\n",
    "            loss_list.append((loss.detach().cpu().item(), loss_positive.detach().cpu().item(), loss_negative.detach().cpu().item()))\n",
    "            avg_time += comp_time\n",
    "            counts += 1\n",
    "    \n",
    "    \n",
    "    #%% md\n",
    "    \n",
    "    # Visualization\n",
    "    \n",
    "    #%%\n",
    "    \n",
    "    def draw_line(plt, epoch_list, data_list, color, label, max_marker=None):\n",
    "        plt.plot(epoch_list, data_list, color=color, label=label)\n",
    "        if max_marker is not None:\n",
    "            max_index = np.argmax(data_list)\n",
    "            val_str = \"{:.4f}\".format(data_list[max_index])\n",
    "            plt.plot(epoch_list[max_index],data_list[max_index], color=color, marker=max_marker)\n",
    "            plt.annotate(val_str,xytext=(-20, 10), textcoords='offset points',\n",
    "                xy=(epoch_list[max_index],data_list[max_index]), color=color)\n",
    "        \n",
    "    import matplotlib.pyplot as plt\n",
    "    # draw_line(plt, epoch_list, auc_score_list, 'r', label=\"auc negative - positive\", max_marker=\"^\")\n",
    "    draw_line(plt, epoch_list, auc_pos_list, 'g', label=\"auc positive\", max_marker=\"^\")\n",
    "    # draw_line(plt, epoch_list, auc_neg_list, 'b', label=\"auc negative\", max_marker=\"^\")\n",
    "    \n",
    "    \n",
    "    plt.ylabel('auc')\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1)\n",
    "    plt.yticks(np.arange(0, 1, step=0.1))\n",
    "    plt.savefig(\"aggdv2.png\", dpi=400)\n",
    "    \n",
    "    #%%\n",
    "    \n",
    "    print(max(auc_pos_list))\n",
    "    \n",
    "    #%%\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "ggd",
   "language": "python",
   "display_name": "GGD"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}